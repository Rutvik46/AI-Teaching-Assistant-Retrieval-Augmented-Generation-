{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEP 775 Final Project Code (Implementation of AI Teaching Assistant)\n",
    "\n",
    "Submitted By Group-4 :\n",
    "\n",
    "Rutvik, Damjibhai Roy â€“ 400490159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auJfJ-aleoun",
    "outputId": "82c7c753-ee4c-4549-cdcf-7eb91c7b4ddf"
   },
   "outputs": [],
   "source": [
    "# Install neccessary libraries\n",
    "\n",
    "!pip install langchain==0.1.14\n",
    "!pip install sentence-transformers==2.6.1\n",
    "!pip install faiss-cpu==1.8.0\n",
    "!pip install pdfminer.six==20231228\n",
    "!pip install llama_cpp_python==0.2.58\n",
    "!pip install tiktoken==0.6.0\n",
    "!pip install gdown==5.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUOq6PU_gr4f"
   },
   "source": [
    "# **1. Load Documents (PDF files of SEP 775 Course Material ) and LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfZG2gxtUZdL",
    "outputId": "93cae8c4-3840-4a66-9a0d-5e1f7cd2c501"
   },
   "outputs": [],
   "source": [
    "# Download llama-2 7B model from gdrive\n",
    "\n",
    "import gdown\n",
    "\n",
    "id='1fJPVfJssRO-PXaHKkxMe4BZ2PS4QwXBn'\n",
    "url = f\"https://drive.google.com/uc?id={id}\"\n",
    "output = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6y-2tzF4UZYY"
   },
   "outputs": [],
   "source": [
    "# Download all pdf files (SEP 775 Course Materials) from gdrive\n",
    "\n",
    "import gdown\n",
    "\n",
    "id=\"1PuOPV-TYxcfxmqwcUqenTSuPlXFVRd-P\"\n",
    "!mkdir '/All_lecture_pdfs'\n",
    "output='/All_lecture_pdfs'\n",
    "documents=gdown.download_folder(id=id,output=output, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Extract Text and Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDptWjKlFyYw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Function to get last modified time of pdf files\n",
    "def get_modified_time(file):\n",
    "  ti_m = os.path.getmtime(file)\n",
    "  m_ti = time.ctime(ti_m)\n",
    "  t_obj = time.strptime(m_ti)\n",
    "  # Transforming the time object to a timestamp of ISO 8601 format\n",
    "  T_stamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", t_obj)\n",
    "  return T_stamp\n",
    "\n",
    "def replace_newlines(text):\n",
    "    # Replace newline and carriage return + line feed characters with spaces\n",
    "    return text.replace('\\n', ' ').replace('\\r\\n', ' ').replace('\\x0c', ' ')\n",
    "\n",
    "def fix_missing_spaces(text):\n",
    "    # Split text into sentences\n",
    "    sentences = text.split('. ')\n",
    "    # Add space after period for each sentence\n",
    "    fixed_text = '. '.join(sentence + (' ' if i < len(sentences) - 1 else '') for i, sentence in enumerate(sentences))\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtEosT3soWgT"
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "# Function to extract text from pdf files\n",
    "def prepare_docs(pdf_docs):\n",
    "    docs = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "      for page_number, page_layout in enumerate(extract_pages(pdf), start=1):\n",
    "        # Extract text from the page layout\n",
    "        text = \"\"\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                text += element.get_text()\n",
    "\n",
    "        text = replace_newlines(text)\n",
    "        text = fix_missing_spaces(text)\n",
    "        doc_page = {'Title': pdf.split(\"\\\\\")[-1] + \" Page No: \" + str(page_number),\n",
    "                    'Last_modified_time': get_modified_time(pdf),\n",
    "                    'Content': text,\n",
    "                    'Source': \"empty_url\"}\n",
    "        docs.append(doc_page)\n",
    "\n",
    "    for doc in docs:\n",
    "        content.append(doc[\"Content\"])\n",
    "        metadata.append({\n",
    "            \"Title\": doc[\"Title\"],\n",
    "            \"Last_modified_time\": doc[\"Last_modified_time\"],\n",
    "            \"Source\": doc[\"Source\"]\n",
    "        })\n",
    "\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "\n",
    "    return content, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrkzPSy8tHHe"
   },
   "outputs": [],
   "source": [
    "# Split extracted text into chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_text_chunks(content, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512,chunk_overlap=15)\n",
    "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Vector Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493,
     "referenced_widgets": [
      "bac2a34dee914176ab9c9170934b058a",
      "c69ede23a6d243579b0ce52801f238c9",
      "66d0fd46e47349cfb375b377819c9848",
      "90cb6f38665f47aaa99c4612c3215325",
      "09bb64c338c64da2bc11cd1ba0ff8259",
      "0474c2e7eb4c4efd8ca5d535de4d9971",
      "2acc5ff149cd45658acba451f580297c",
      "2ee6e338d17d43f690ddf5147a2a1f1d",
      "eff48de46fbf46689118c16470192d6f",
      "4bf7aa45d6e14558a2153f9c30eccb52",
      "06e798f7e18c42c2a06ea15aeb1962a5",
      "0a15d173d054417a890c95a564dde254",
      "be459bed75e24c2485ab385ac87f6655",
      "9eb26a093daa4b748ce45ed73a2ab392",
      "6cb41986298d417d9ea7d81187dd1ecc",
      "80117347b8dc4234a5ed229cec5784f4",
      "b1d98004372a43aea53361fb91d38c50",
      "bd9ccbd19ac74ecb956c3f16dfc68e1d",
      "767cd66559d84c80a6ebae188a2b185f",
      "6c8140a0219d4cbeac949fa1767a4372",
      "b7d639594ca246279386037b190b5c19",
      "d827e8677d0c4ffdbd18b918b8872d76",
      "d2deb0f40a3b4628b4fd09e382578b6b",
      "db8f987e2077462884c0ff4ee1d07826",
      "dc2515e50c594937ad6129d157253517",
      "4d4039986e67485e99fa3feb7b33c37d",
      "1d66ecf7e98548e5be706c871d0d3abf",
      "03688f697e1742969c309ac64e60cf40",
      "eaf77693e5dd4a92bcfc67a81da7be8f",
      "90d467cc3f704e0495bc400ab63c7817",
      "f5f7ca16f28643fea6b22dea47b71d75",
      "b710119256f943ab8be10a5a2304ee00",
      "42b7625b339547f79a0fd5c2b86a1915",
      "2da1d9a7c5a24ac3bb7a164a4d26132b",
      "17cde5489721459ab3095b3438b8dd30",
      "eb079b3d7bb742b1922fd0e98e773afa",
      "6c38fd11bab74757b65cf800441d7959",
      "085bbe731e624dbea5c37f3c6c3ff027",
      "05eb11b1c8d34fbbba8b3aa23addda67",
      "3902466eadc74bf4b4c925a85943103e",
      "c26d60fa03a54e77a0c8461306aacbb7",
      "8866d849e4804207bf52e26e1a91b042",
      "c8612ebb11fd41baa175adf749c005c4",
      "7e6f85297cf942c6bbb602e8584f342e",
      "6645be905a9d46138b8d9c2cdbeb621a",
      "e3843ba7f79c4e6891d2804c80d16b73",
      "a3f6a15017f243b2afafce2ccaec52ea",
      "c3bfb1ca9b7745ab8f4787638d7e1ab5",
      "6c8508497d344db4a3c6d90384a22ca4",
      "0b6fe3c84f8244b0bcb75dc40fde3608",
      "11e6addfbb4644bab2b79988b55d217f",
      "5ae869d0f8234470b6e6edcfa4185e7e",
      "fc3730c9ee48402a87d677d56d49a69f",
      "89ecab446f274dba88961bd42cb5f9ba",
      "f3fe13f9a57846deb430d9b4872f39aa",
      "bb23d015017b47c8a4a9c6be91fffd6f",
      "459a1edc2bc946579f03dd4513188fee",
      "99001356f5ac443ba89efb1436b67939",
      "a9de02a657004fa8ac1620f508332f63",
      "c4491455b027465d9d948569cb2a2a41",
      "7ad15ad705054987ab29c5d3da861d9a",
      "1b352ac0dd69447abf8089b4138cf2d2",
      "0013bc1b3b864044b71240c724965c1d",
      "e8175c74091a4acea1ab23a5cb289d24",
      "ab020050053c486c91c9744effc03de9",
      "f989fe6bbe384c3b89e3f86341ec1874",
      "129440252d3242e5974366e54fcc5303",
      "5c6c07d1026a4fa3b9bb1709e374eeac",
      "1e21a3bd82db4619991f1d3223568097",
      "cd578c54db244fed8feef299bbbc7297",
      "7c89d1541c5a4b78bd9964e70f9c879c",
      "35ae44f1f1e14e6fa56879ddbc258d82",
      "4eb704cb39b14513b731f592194cd7c1",
      "859a772deb614ce49343e7d51d1c29d2",
      "0e7396cf7a5c4416bccd28b3a8866c81",
      "271dd40578a94725a8155e3fff86bc5a",
      "3c3a5e08842d4699909d0f81c070c445",
      "bc76fcea12064f2198194b4d2bcd1243",
      "84fdadea8ece4facb14a37d710945e65",
      "9d260498c62e44a9a0ef47b72cc9494a",
      "5238ae29bb1b4714a7a5f89bc72cee7f",
      "c40b47027e1e4947a2cb3f8318ec8df7",
      "02db1b3f8141499180c7c3c51e29cb1a",
      "882cd86446d64be3b0165fce9fd4d700",
      "a5756626f28b4d069a99cb994a18a01e",
      "22e4d46616274decbf3daf2112b5775a",
      "4ad36ebfbd334d75ac81afee262bebb9",
      "80f1efe9f36149159100498afc27bccc",
      "55140878128d43439f092c070a9608a6",
      "ca4de98fc176447780d2fa433985c6a6",
      "6fdebd42cee443a381f0d9dcd47c1fc9",
      "815c3ac4dd4645d49932ba273070b3cf",
      "a019b7383ae74925869069fd0865c682",
      "1c463bf1628e46fd8864062460ec5e28",
      "6fd2eaeb35694584aa28d812eaec2964",
      "72ff38aa2071428d8f9324667c009f40",
      "e33e7938d0ac4bf6af5af6201f72f102",
      "608ec8b44db645dea6497a37bf422f30",
      "9311b43eeef64c8a9cf1f565670ed87b",
      "6d5b013a4c7749a680b67821e09f709e",
      "414f2d7d3b0e44e2a345124d6c3fc27d",
      "85f6f9e9463b40ea82a7e713e5eb7dd1",
      "2725c8c585544b5199ac547fb18cdddd",
      "dc7d8986f9f6452e9a6f1b1d4faa53ba",
      "ddce81a611fd430cb9085772bb9781ec",
      "5bfe6b1bf307416e935cfd5c295acb36",
      "6a3e255723634dfda9e887da7814d1e4",
      "cbedda34121149bf8dc03b172cd0de2d",
      "0edabb8666cc4e78af2e4d46b4e61d9a",
      "0504295cb5f54298b0cbce51d49ca031",
      "20845afbf1ab4bd0b503f25de5ea4c36",
      "c8c6d3f16aa04a31acfea9a5448fddfe",
      "4c67f292dc9b43d6bbfa2823f7b7a6f6",
      "b7b320be4916400987907d5d75180e9e",
      "79378caee4924637985f41a2237f6d26",
      "8149d613bfe046bd9289ea42fa9e25aa",
      "e12cd8b8f33f4f5293d2ba9bb06b0be1",
      "c4bcfe23537d4198bf27d2ee4c0ff42e",
      "0348b4ac1cb74962a86748743c21f83b",
      "51a16f2d5f5c4a1d96f0a65bc4ce3fd7",
      "b2052ee028d845fb97338874fddcd0d0"
     ]
    },
    "id": "zESd6FNK2Otq",
    "outputId": "4ce68cbd-c0de-4dd2-9b9c-1862e0c11b9c"
   },
   "outputs": [],
   "source": [
    "# Create Vector database\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def ingest_into_vectordb(split_docs):\n",
    "    embeddings = embedding_model\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    print(\"Vector database is created\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDlSNte16AWr",
    "outputId": "23089a68-cd15-45d0-ab79-3e49376be122"
   },
   "outputs": [],
   "source": [
    "content, metadata = prepare_docs(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkLe_3c66AY9",
    "outputId": "f129859f-6e85-4fcc-f316-f5184b8dcb79"
   },
   "outputs": [],
   "source": [
    "split_docs = get_text_chunks(content, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSbaPdMg6Ab0"
   },
   "outputs": [],
   "source": [
    "vectordb=ingest_into_vectordb(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0pnjRHDsW1U"
   },
   "source": [
    "# **4. Database Retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6T1duYfUtF_"
   },
   "outputs": [],
   "source": [
    "# Set retriver\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcSsFL9_UFBu",
    "outputId": "c3a10e45-5a25-4959-a714-a2cc1d0e429d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test a user query\n",
    "\n",
    "user_query=\"What is RNN?\"\n",
    "query_embedding = embedding_model.embed_query(user_query)\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5c5Zc8DUtWk",
    "outputId": "32b8d9ce-806b-45ef-ed83-4c60a283e8d6"
   },
   "outputs": [],
   "source": [
    "# Check retriver\n",
    "\n",
    "docs = retriever.get_relevant_documents(user_query)\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "  print(f\"Retrived Chunk - {i+1} :\",docs[i].page_content)\n",
    "  print(\" \")\n",
    "  print(\"Title:\",doc.metadata[\"Title\"])\n",
    "  print(\"Last_modified_time:\",doc.metadata[\"Last_modified_time\"])\n",
    "  print(\"Source:\",doc.metadata[\"Source\"])\n",
    "  print(\"-\"*40)\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcWdiDmvyePz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg_vWxv218Cs"
   },
   "source": [
    "# **5. RAG Conversational Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwhTes3_2H5W"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# callback manager for word to word streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Load Llama-2 Model from directory\n",
    "\n",
    "llama_llm = LlamaCpp(\n",
    "model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "n_gpu_layers=15000, \n",
    "n_threads=6,     # Number of CPU core\n",
    "n_batch=512,\n",
    "temperature=0.7,\n",
    "f16_kv=True,\n",
    "max_tokens=512,\n",
    "top_p=0.95,\n",
    "callback_manager=callback_manager,\n",
    "n_ctx=4096,\n",
    "verbose=True,\n",
    "streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Prompt Template and retrieved document's citation (Define a fuction to cite retrived chunks' sources with LLM's response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIbYtktcu1Pc"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Set up conversation memory to save chat history\n",
    "memory=ConversationBufferWindowMemory(k=2,memory_key=\"chat_history\", return_messages=True, input_key=\"question\")\n",
    "\n",
    "# Function to set up langchain chain for sequence of call\n",
    "def get_conversational_chain():\n",
    "\n",
    "    prompt_template =\"\"\"\n",
    "    You are a helpful Teaching Assistant of the McMaster University and your name is 'Mac AI Assistant'.\\n\n",
    "    This is the conversation between a student and and 'Mac AI Assistant\". your job is to answer the student's question.\\n\n",
    "    The question can be a new question or follow up. So, you must check the chat histroy given below before you answer the question.\\n\n",
    "    You must answer student's the question based on only context given below.\\n\n",
    "    If the question can not be answered using the information provided in the context, must answer with I don't know, don't try to make up an answer.\\n\n",
    "    Give answers in natural form, without giving context as of what you're doing internally.\\n\n",
    "    Use three sentences maximum. Keep the answer as concise as possible.\\n\n",
    "    If user question is more general for eaxmple 'Hi', 'Hi there!, 'Thanks', or 'How are you!', then asnwer them like a personal assistant of an user\\n\n",
    "    Always begin your answer with this dialog format:\\n 'Mac AI Assistant: <your_Answer>' \\n\\n\n",
    "\n",
    "    context:\\n{context}\\n\n",
    "\n",
    "    Student's Question: \\n{question}\\n\n",
    "\n",
    "    Chat history: \\n{chat_history}\\n\n",
    "\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    # Langchain Prompt template to configure prompt variable\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\",\"chat_history\"])\n",
    "\n",
    "    # Chain for sequence of call\n",
    "    chain = load_qa_chain(llama_llm, chain_type=\"stuff\", prompt=prompt, memory=memory)\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onHI_3_etw4a"
   },
   "outputs": [],
   "source": [
    "# Function for user input\n",
    "\n",
    "def user_input(user_question):\n",
    "  docs = retriever.get_relevant_documents(user_question)\n",
    "  chain = get_conversational_chain()\n",
    "  response = chain.invoke({\"input_documents\": docs, \"question\": user_question})\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cite retrived resources    \n",
    "\n",
    "def citation_function(response):\n",
    "    print('\\nCitation:')\n",
    "    for source in response[\"input_documents\"]:\n",
    "        print(source.metadata['Title'],\" \",\"Source URL:\",source.metadata['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB2zgLHFtw-H",
    "outputId": "6beeed19-827d-46e7-a626-40382437cd5c"
   },
   "outputs": [],
   "source": [
    "# Create a loop to continuously interact with QA chain ( TO exit the loop - Enter exit)\n",
    "\n",
    "while True:\n",
    "    user_query_=input(\"User: \")\n",
    "    print(\"User: \",user_query_)\n",
    "    if user_query_.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "    response=user_input(user_query_)\n",
    "    citation_function(response)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Include citation within the prompt template (Citation done By LLM according to instruction inside prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Set up conversation memory\n",
    "memory=ConversationBufferWindowMemory(k=2,memory_key=\"chat_history\", return_messages=True, input_key=\"question\")\n",
    "\n",
    "def get_conversational_chain():\n",
    "\n",
    "    prompt_template =\"\"\"\n",
    "    You are a helpful Teaching Assistant of the McMaster University and your name is 'Mac AI Assistant'.\\n\n",
    "    This is the conversation between a student and and 'Mac AI Assistant\". your job is to answer the student's question.\\n\n",
    "    The question can be a new question or follow up. So, you must check the chat histroy given below before you answer the question.\\n\n",
    "    You must answer student's the question based on only context given below.\\n\n",
    "    If the question can not be answered using the information provided in the context, must answer with I don't know, don't try to make up an answer.\\n\n",
    "    Give answers in natural form, without giving context as of what you're doing internally.\\n\n",
    "    Use three sentences maximum. Keep the answer as concise as possible.\\n\n",
    "    If an answer to the question is provided uisng the context data, it must be annotated with a citation at the end of the answer. should use the following format to cite all three sources specified in the Sources after context data. \"\\nCitation: \\nsource-1.pdf Page No: xx-1  Source URL: xxx-1 \\nsource-2.pdf Page No: xx-2  Source URL: xxx-2 \\nsource-3.pdf Page No: xx-3  Source URL: xxx-3\".\n",
    "    If user question is more general for eaxmple 'Hi', 'Hi there!, 'Thanks', or 'How are you!', then asnwer them like a personal assistant of an user and do not need citation in the asnwer\\n\n",
    "    Always begin your answer with this dialog format:\\n'Mac AI Assistant: <your_Answer>' \\n\\n\n",
    "\n",
    "    \"\"context:\\n{context}\\n\\n\"\"\n",
    "\n",
    "    \"\"Chat history: \\n{chat_history}\\n\\n\"\"\n",
    "\n",
    "    \"\"Student's Question: \\n{question}\\n\\n\"\"\n",
    "\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    # Langchain Prompt template to configure prompt variable\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\",\"chat_history\"])\n",
    "\n",
    "    # chain for sequence of call\n",
    "    chain = load_qa_chain(llama_llm, chain_type=\"stuff\", prompt=prompt, memory=memory,verbose=False)\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Function to add retrievd text and metadata to put into prompt\n",
    "def Add_text_with_metadata(docs):\n",
    "    text=\"\"\n",
    "    data=\"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        text+=doc.page_content+\"\\n\"\n",
    "        data+=doc.metadata['Title']+\" Source URL: \"+ doc.metadata['Source']+\"\\n\"\n",
    "\n",
    "    final_text=text +\"Sources: \\n\"+data\n",
    "    doc =  Document(page_content=f\"{final_text}\")\n",
    "\n",
    "    return [doc]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for user input\n",
    "\n",
    "def user_input(user_question):\n",
    "  retrived_docs = retriever.get_relevant_documents(user_question)\n",
    "  docs=Add_text_with_metadata(retrived_docs)\n",
    "  chain = get_conversational_chain()\n",
    "  response = chain.invoke({\"input_documents\": docs, \"question\": user_question})\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop to continuously interact with QA chain ( TO exit the loop - Enter exit)\n",
    "\n",
    "while True:\n",
    "    user_query_=input(\"User: \")\n",
    "    print(\"User: \",user_query_)\n",
    "    if user_query_.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "    response=user_input(user_query_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
